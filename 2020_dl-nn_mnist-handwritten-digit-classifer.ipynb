{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020_dl-nn_mnist-handwritten-digit-classifer\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This is a deep neural network that I am building from my notes from this class: [Udacity - Intro to Deep Learning with Pytorch](https://classroom.udacity.com/courses/ud188/lessons/c5706f76-0e30-4b48-b74e-c19fafc33a75/concepts/0d735345-f080-49c4-8fbf-a68012923f50). I want to reinforce what I learned from Lessons 4 and 5 by building a complete (basic) network using only my notes and the Pytorch documentation as references.\n",
    "\n",
    "## Plan\n",
    "\n",
    "- Transform and load MNIST handwritten digits data\n",
    "- Define a neural network class\n",
    "- Select an error function and optimizer for the network\n",
    "- Train the model with validation\n",
    "  - Print training and testing loss each epoch\n",
    "  - Graph?\n",
    "- Save the model\n",
    "- Load the model from file\n",
    "- Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download, Transform, and Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_data = datasets.MNIST(\"./2020_data_mnist\", train=True, transform=transform, download=True)\n",
    "test_data = datasets.MNIST(\"./2020_data_mnist\", train=False, transform=transform, download=False)\n",
    "\n",
    "# Percent of training data to split off for validation\n",
    "valid_size = 0.2\n",
    "\n",
    "data_len = len(train_data)\n",
    "indices = list(range(data_len))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * data_len))\n",
    "valid_indices, train_indices = indices[:split], indices[split:]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(valid_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, sampler=train_sampler)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=64, sampler=valid_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 48000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# We will use these values to average the loss values during training, so want to see them now\n",
    "print(images.size(0), len(train_loader.sampler))\n",
    "\n",
    "image = images[0].view(28, 28)\n",
    "plt.imshow(image)\n",
    "plt.title(int(labels[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_features = 28*28 # Number of pixels in a 28x28 image\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        hidden_layers = 512 # Define size of hidden layers\n",
    "        outputs = 10 # We have ten outputs, because there are 10 classes: integers 0 - 9\n",
    "        \n",
    "        # Define a network with three linear layers\n",
    "        self.fc1 = nn.Linear(input_features, hidden_layers)\n",
    "        self.fc2 = nn.Linear(hidden_layers, hidden_layers)\n",
    "        self.fc3 = nn.Linear(hidden_layers, outputs)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Flatten the input image\n",
    "        x = x.view(-1, input_features)\n",
    "        \n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        \n",
    "        # The softmax function will be applied by the loss function\n",
    "        # nn.CrossEntropyLoss(), so we don't need it here\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = Classifier()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_funct = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining loss: 1.8639494188626606 \tValidation loss: 1.1863089942932128\n",
      "Epoch: 2 \tTraining loss: 0.8901772654851278 \tValidation loss: 0.6020880839029948\n",
      "Epoch: 3 \tTraining loss: 0.5922448638280233 \tValidation loss: 0.46278220462799075\n",
      "Epoch: 4 \tTraining loss: 0.49522847537199655 \tValidation loss: 0.4041187928120295\n",
      "Epoch: 5 \tTraining loss: 0.44148444962501526 \tValidation loss: 0.36484241326649985\n",
      "Epoch: 6 \tTraining loss: 0.4084615867137909 \tValidation loss: 0.3427647197643916\n",
      "Epoch: 7 \tTraining loss: 0.3848771220048269 \tValidation loss: 0.3262525362968445\n",
      "Epoch: 8 \tTraining loss: 0.36506515250603355 \tValidation loss: 0.3136483033498128\n",
      "Epoch: 9 \tTraining loss: 0.3483230048120022 \tValidation loss: 0.3007921762863795\n",
      "Epoch: 10 \tTraining loss: 0.33554158528645833 \tValidation loss: 0.28934588420391083\n",
      "Epoch: 11 \tTraining loss: 0.32288234227895735 \tValidation loss: 0.2814884490172068\n",
      "Epoch: 12 \tTraining loss: 0.3123127380112807 \tValidation loss: 0.27329138306776685\n",
      "Epoch: 13 \tTraining loss: 0.3007120468417803 \tValidation loss: 0.26372718371947607\n",
      "Epoch: 14 \tTraining loss: 0.2920074890951316 \tValidation loss: 0.25595677026112873\n",
      "Epoch: 15 \tTraining loss: 0.28365173303087554 \tValidation loss: 0.24955359665552776\n",
      "Epoch: 16 \tTraining loss: 0.27419201599558196 \tValidation loss: 0.24297870318094889\n",
      "Epoch: 17 \tTraining loss: 0.2688476818303267 \tValidation loss: 0.23580352765321733\n",
      "Epoch: 18 \tTraining loss: 0.2598436614473661 \tValidation loss: 0.23084126538038255\n",
      "Epoch: 19 \tTraining loss: 0.2521803684880336 \tValidation loss: 0.2248679089943568\n",
      "Epoch: 20 \tTraining loss: 0.24659553541739782 \tValidation loss: 0.21926667487621307\n",
      "Epoch: 21 \tTraining loss: 0.2410156413813432 \tValidation loss: 0.21339860606193542\n",
      "Epoch: 22 \tTraining loss: 0.23331694677472115 \tValidation loss: 0.20836775066455204\n",
      "Epoch: 23 \tTraining loss: 0.22802034135659535 \tValidation loss: 0.20213784754276276\n",
      "Epoch: 24 \tTraining loss: 0.22134036735693613 \tValidation loss: 0.1975816513299942\n",
      "Epoch: 25 \tTraining loss: 0.21570672249794007 \tValidation loss: 0.1938197785615921\n",
      "Epoch: 26 \tTraining loss: 0.21109616876145204 \tValidation loss: 0.18911464281876883\n",
      "Epoch: 27 \tTraining loss: 0.2045951454391082 \tValidation loss: 0.1852819121082624\n",
      "Epoch: 28 \tTraining loss: 0.20127117837468783 \tValidation loss: 0.1807303496003151\n",
      "Epoch: 29 \tTraining loss: 0.195968749816219 \tValidation loss: 0.1767273128827413\n",
      "Epoch: 30 \tTraining loss: 0.19170537675420443 \tValidation loss: 0.17345447979370754\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "# Losses per epoch\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "# Minimum validation loss\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    # Loss per epoch\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = loss_funct(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    train_losses.append(train_loss)\n",
    "        \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    for images, labels in valid_loader:\n",
    "        output = model(images)\n",
    "        loss = loss_funct(output, labels)\n",
    "        valid_loss += loss.item()*images.size(0)\n",
    "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if valid_loss > valid_loss_min:\n",
    "        break\n",
    "    else:\n",
    "        valid_loss_min = valid_loss\n",
    "        \n",
    "    # Telemetry\n",
    "    print(\"Epoch:\", e+1, \"\\tTraining loss:\", train_loss, \"\\tValidation loss:\", valid_loss)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
